{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy's small English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load semantic analysis model (using DistilBERT for simplicity)\n",
    "semantic_model = pipeline(\"text-classification\", model=\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset (replace 'your_books_dataset.csv' with the path to your dataset)\n",
    "df = pd.read_csv(\"C:/Users/DELL/Documents/NLP/nlp project/Amazon_Books_Scraping/Books_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for syntax verification using POS tagging and dependency parsing\n",
    "def syntax_verification(text):\n",
    "    doc = nlp(text)\n",
    "    feedback = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'ROOT' and token.pos_ not in ['VERB', 'AUX']:\n",
    "            feedback.append(f\"Possible syntax error at '{token.text}' - expected a verb.\")\n",
    "    return feedback if feedback else \"The sentence is syntactically correct.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for semantic verification using a pre-trained model\n",
    "def semantic_verification(text):\n",
    "    result = semantic_model(text)[0]\n",
    "    if result['label'] == 'LABEL_1':  # Assuming LABEL_1 means semantically correct\n",
    "        return \"The sentence makes sense.\"\n",
    "    else:\n",
    "        return \"The sentence may not be semantically correct.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get part-of-speech tagging\n",
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.pos_) for token in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform NER (Named Entity Recognition)\n",
    "def named_entity_recognition(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform chunking (Noun/Verb Phrases)\n",
    "def chunking(text):\n",
    "    doc = nlp(text)\n",
    "    return [(chunk.text, chunk.root.dep_) for chunk in doc.noun_chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for dependency parsing\n",
    "def dependency_parsing(text):\n",
    "    doc = nlp(text)\n",
    "    return [(token.text, token.dep_, token.head.text) for token in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch book recommendations from your dataset\n",
    "def fetch_books_from_dataset(query):\n",
    "    # Filter dataset based on the query\n",
    "    matched_books = df[df['Description'].str.contains(query, case=False, na=False) |\n",
    "                       df['Title'].str.contains(query, case=False, na=False)]\n",
    "    \n",
    "    if not matched_books.empty:\n",
    "        return [{\"title\": row['Title'], \"author\": row['Author']} for idx, row in matched_books.iterrows()]\n",
    "    else:\n",
    "        return [\"No matching books found in dataset.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch book data from Google Books API\n",
    "def fetch_books_from_google(query):\n",
    "    google_api_url = f\"https://www.googleapis.com/books/v1/volumes?q={query}\"\n",
    "    response = requests.get(google_api_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        books = response.json().get('items', [])\n",
    "        results = [{\"title\": book[\"volumeInfo\"][\"title\"], \n",
    "                    \"author\": book[\"volumeInfo\"].get(\"authors\", [\"Unknown\"])[0]} for book in books[:5]]\n",
    "        return results if results else [\"No matching books found in Google Books API.\"]\n",
    "    return [\"Failed to fetch data from Google Books API.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch book data from Open Library API\n",
    "def fetch_books_from_open_library(query):\n",
    "    open_library_api_url = f\"https://openlibrary.org/search.json?q={query}\"\n",
    "    response = requests.get(open_library_api_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        books = response.json().get('docs', [])\n",
    "        results = [{\"title\": book[\"title\"], \n",
    "                    \"author\": book.get(\"author_name\", [\"Unknown\"])[0]} for book in books[:5]]\n",
    "        return results if results else [\"No matching books found in Open Library API.\"]\n",
    "    return [\"Failed to fetch data from Open Library API.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit Interface\n",
    "def main():\n",
    "    st.title(\"NLP Exam Preparation Chatbot with Dataset & API Integration\")\n",
    "\n",
    "    user_input = st.text_input(\"Ask me a question or input a sentence:\")\n",
    "\n",
    "    if st.button(\"Analyze\"):\n",
    "        if user_input:\n",
    "            # Step 1: Syntax Verification\n",
    "            syntax_feedback = syntax_verification(user_input)\n",
    "            st.subheader(\"Syntax Verification\")\n",
    "            st.write(syntax_feedback)\n",
    "\n",
    "            # Step 2: Semantic Verification\n",
    "            semantic_feedback = semantic_verification(user_input)\n",
    "            st.subheader(\"Semantic Verification\")\n",
    "            st.write(semantic_feedback)\n",
    "\n",
    "            # Step 3: POS Tagging\n",
    "            pos_tags = pos_tagging(user_input)\n",
    "            st.subheader(\"Part-of-Speech Tagging\")\n",
    "            for word, tag in pos_tags:\n",
    "                st.write(f\"{word}: {tag}\")\n",
    "\n",
    "            # Step 4: Named Entity Recognition\n",
    "            entities = named_entity_recognition(user_input)\n",
    "            st.subheader(\"Named Entity Recognition\")\n",
    "            for entity, label in entities:\n",
    "                st.write(f\"{entity}: {label}\")\n",
    "\n",
    "            # Step 5: Chunking\n",
    "            chunks = chunking(user_input)\n",
    "            st.subheader(\"Chunking\")\n",
    "            for chunk, dep in chunks:\n",
    "                st.write(f\"{chunk}: {dep}\")\n",
    "\n",
    "            # Step 6: Dependency Parsing\n",
    "            dependencies = dependency_parsing(user_input)\n",
    "            st.subheader(\"Dependency Parsing\")\n",
    "            for word, dep, head in dependencies:\n",
    "                st.write(f\"{word}: {dep} â†’ {head}\")\n",
    "\n",
    "            # Step 7: Book Recommendations from Dataset\n",
    "            st.subheader(\"Book Recommendations from Dataset\")\n",
    "            books_from_dataset = fetch_books_from_dataset(user_input)\n",
    "            for book in books_from_dataset:\n",
    "                if isinstance(book, dict):\n",
    "                    st.write(f\"Title: {book['title']}, Author: {book['author']}\")\n",
    "                else:\n",
    "                    st.write(book)\n",
    "\n",
    "            # Step 8: Book Recommendations from Google Books API\n",
    "            st.subheader(\"Book Recommendations from Google Books API\")\n",
    "            books_from_google = fetch_books_from_google(user_input)\n",
    "            for book in books_from_google:\n",
    "                if isinstance(book, dict):\n",
    "                    st.write(f\"Title: {book['title']}, Author: {book['author']}\")\n",
    "                else:\n",
    "                    st.write(book)\n",
    "\n",
    "            # Step 9: Book Recommendations from Open Library API\n",
    "            st.subheader(\"Book Recommendations from Open Library API\")\n",
    "            books_from_open_library = fetch_books_from_open_library(user_input)\n",
    "            for book in books_from_open_library:\n",
    "                if isinstance(book, dict):\n",
    "                    st.write(f\"Title: {book['title']}, Author: {book['author']}\")\n",
    "                else:\n",
    "                    st.write(book)\n",
    "        else:\n",
    "            st.write(\"Please enter a sentence or question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 12:16:12.973 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.196 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-12-10 12:16:13.197 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.198 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.200 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.201 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.201 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.202 Session state does not function when running a script without `streamlit run`\n",
      "2024-12-10 12:16:13.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.208 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.208 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-10 12:16:13.210 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
